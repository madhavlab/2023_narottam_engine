{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pred_truth(teacher_id):\n",
    "    y_pred1=np.load(\"y_pred_1_\"+teacher_id+\".npy\",allow_pickle=True)\n",
    "    y_true1=np.load(\"y_true_1_\"+teacher_id+\".npy\",allow_pickle=True)\n",
    "    y_pred2=np.load(\"y_pred_2_\"+teacher_id+\".npy\",allow_pickle=True)\n",
    "    y_true2=np.load(\"y_true_2_\"+teacher_id+\".npy\",allow_pickle=True)\n",
    "    return y_pred1,y_true1,y_pred2,y_true2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(arr):\n",
    "    x=[list(j) for i, j in groupby(arr)]\n",
    "    arr=[]\n",
    "    s=0\n",
    "    for i in range(len(x)):\n",
    "        arr.append([s,s+len(x[i])-1,x[i][0]])\n",
    "        s+=len(x[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothening(arr,k):\n",
    "    y=[list(j) for i, j in groupby(arr)]\n",
    "    arr_n=[]\n",
    "    for i in range(len(y)):\n",
    "        if len(y[i])<=k and i==0:\n",
    "            for j in y[i]:\n",
    "                arr_n.append(j)\n",
    "        elif len(y[i])<=k and i==(len(y)-1):\n",
    "            for j in [y[i-1][0]]:\n",
    "                arr_n.append(j)\n",
    "        elif len(y[i])<=k:\n",
    "            for j in [y[i+1][0]]:\n",
    "                arr_n.append(j)  \n",
    "        else:\n",
    "            for j in y[i]:\n",
    "                arr_n.append(j)            \n",
    "    return arr_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_timestamp(arr):\n",
    "    new_arr=[]\n",
    "    for i in arr:\n",
    "        s=i[0]*0.01\n",
    "        e=(i[1]*0.01)+0.01\n",
    "        new_arr.append([np.round(s,2),np.round(e,2),i[2]])   \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_labels(arr,label):\n",
    "    new_arr=[]\n",
    "    for i in arr:\n",
    "        if i[-1]==0:\n",
    "            new_arr.append([i[0],i[1],\"N\"])\n",
    "        else:\n",
    "            new_arr.append([i[0],i[1],label])\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersection(lst1,lst2):\n",
    "    intersection=np.logical_or(lst1,lst2)\n",
    "    intersection=intersection+0\n",
    "    return np.array(intersection).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tp_tn(pred,true,c):\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for i in range(len(pred)):\n",
    "        if i-c<0:\n",
    "            s=0\n",
    "            e=i+c+1\n",
    "            t=true[s:e]\n",
    "        else:\n",
    "            t=true[i-c:c+i+1]\n",
    "        if pred[i] in t and pred[i]==1:\n",
    "            tp+=1\n",
    "        elif pred[i] in t and pred[i]==0:\n",
    "            tn+=1\n",
    "        elif pred[i] not in t and pred[i]==1:\n",
    "            fp+=1\n",
    "        elif  pred[i] not in t and pred[i]==0:\n",
    "            fn+=1\n",
    "    return tp,tn,fp,fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred,true,c):\n",
    "    tp,tn,fp,fn=find_tp_tn(pred,true,c)\n",
    "    p=tp/(tp+fp)\n",
    "    r=tp/(tp+fn)\n",
    "    a=(tp+tn)/(tp+tn+fp+fn+math.ulp(1.0))\n",
    "    f1=2*((p*r)/(p+r))\n",
    "    return p,r,a,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(pred,true,eta,t_collar):\n",
    "    TP,TN,FP,FN = (0,0,0,0)\n",
    "    event_id_pred = 0\n",
    "    for event_id_true in tqdm(range(len(true))):\n",
    "        if event_id_true==0:\n",
    "            onset_true = float(true.iloc[event_id_true,0]) #a\n",
    "            offset_true = float(true.iloc[event_id_true,1])+t_collar #b\n",
    "        elif event_id_true==len(true)-1:\n",
    "            onset_true = float(true.iloc[event_id_true,0])-t_collar #a\n",
    "            offset_true = float(true.iloc[event_id_true,1]) #b\n",
    "        else:\n",
    "            onset_true = float(true.iloc[event_id_true,0])-t_collar #a\n",
    "            offset_true = float(true.iloc[event_id_true,1])+t_collar #b\n",
    "        gt = true.iloc[event_id_true,-1]\n",
    "        delta = eta*(offset_true-onset_true) #overlap_tolerance\n",
    "        onset_pred = int(onset_true*100)\n",
    "        offset_pred = int(offset_true*100)\n",
    "        event_occur_frequency = sum(pred[onset_pred:offset_pred])\n",
    "        #print(event_occur_frequency)\n",
    "        if event_occur_frequency>delta:\n",
    "            if gt != 'N':\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        else:\n",
    "            if gt != 'N':\n",
    "                FN+=1\n",
    "            else:\n",
    "                TN+=1\n",
    "                \n",
    "    accuracy = (TP)/(TP+FP+TN+FN)\n",
    "    precision = (TP)/(TP+FP)\n",
    "    recall = (TP)/(TP+FN)\n",
    "    f1 = (2*precision*recall)/(precision+recall)\n",
    "    print(f'TP- {TP}, FP- {FP}, FN- {FN}, FP- {FP}')\n",
    "    print(f'The Accuracy is {100*accuracy}%.')\n",
    "    print(f'The Precision is {100*precision}%.')\n",
    "    print(f'The Recall is {100*recall}%.')\n",
    "    print(f'The F1-score is {100*f1}%.')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(teacher_id = '002', kappa = 1,metrics_for = 'freq',t_collar = 0, eta = 0.2):\n",
    "    y_pred1,y_true1,y_pred2,y_true2 = load_pred_truth(teacher_id=teacher_id)\n",
    "    if metrics_for == 'freq':\n",
    "        metrics_for = 'F'\n",
    "    else:\n",
    "        metrics_for = 'A'\n",
    "    y_pred1,y_pred2 = smoothening(y_pred1,k = kappa),smoothening(y_pred2,k = kappa)\n",
    "    y_true1,y_true2=group(y_true1),group(y_true2)\n",
    "    y_true1,y_true2 = convert_to_labels(y_true1,metrics_for),convert_to_labels(y_true2,metrics_for)\n",
    "    y_true1 = pd.DataFrame(convert_to_timestamp(y_true1),columns = ['Onset','Offset','Mistake'])\n",
    "    y_true2 = pd.DataFrame(convert_to_timestamp(y_true2),columns = ['Onset','Offset','Mistake'])\n",
    "    #return y_pred1, y_true1\n",
    "    if metrics_for == 'F':\n",
    "        accuracy, precision, recall, f1 = get_metrics(pred = y_pred1,true = y_true1,\n",
    "                                                      eta = eta, t_collar = t_collar)\n",
    "    else:\n",
    "        accuracy, precision, recall, f1 = get_metrics(pred = y_pred2,true = y_true2,\n",
    "                                                      eta = eta, t_collar = t_collar)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_segment_wise(teacher_id,c,k):\n",
    "    y_pred1,y_true1,y_pred2,y_true2=load_pred_truth(teacher_id)\n",
    "    print(\"Results Before Smoothening- \")\n",
    "    p1,r1,a1,f11=evaluate(y_pred1,y_true1,c)\n",
    "    print(\"Frequency-\")\n",
    "    print(\"Accuracy- \",a1)\n",
    "    print(\"Precision-\",p1)\n",
    "    print(\"Recall-\",r1)\n",
    "    print(\"F1 Score- \",f11)\n",
    "    print(\"\\n\")\n",
    "    p2,r2,a2,f12=evaluate(y_pred2,y_true2,c)\n",
    "    print(\"Amplitude-\")\n",
    "    print(\"Accuracy- \",a2)\n",
    "    print(\"Precision-\",p2)\n",
    "    print(\"Recall-\",r2)\n",
    "    print(\"F1 Score- \",f12)\n",
    "    print(\"\\n\")\n",
    "    y_pred1,y_pred2=smoothening(y_pred1,k),smoothening(y_pred2,k)\n",
    "    print(\"Results After Smoothening- \")\n",
    "    p1,r1,a1,f11=evaluate(y_pred1,y_true1,c)\n",
    "    print(\"Frequency-\")\n",
    "    print(\"Accuracy- \",a1)\n",
    "    print(\"Precision-\",p1)\n",
    "    print(\"Recall-\",r1)\n",
    "    print(\"F1 Score- \",f11)\n",
    "    print(\"\\n\")\n",
    "    p2,r2,a2,f12=evaluate(y_pred2,y_true2,c)\n",
    "    print(\"Amplitude-\")\n",
    "    print(\"Accuracy- \",a2)\n",
    "    print(\"Precision-\",p2)\n",
    "    print(\"Recall-\",r2)\n",
    "    print(\"F1 Score- \",f12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_id=\"002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Before Smoothening- \n",
      "Frequency-\n",
      "Accuracy-  0.7558651026392962\n",
      "Precision- 0.6696132750508116\n",
      "Recall- 0.8341021016855101\n",
      "F1 Score-  0.7428611141140676\n",
      "\n",
      "\n",
      "Amplitude-\n",
      "Accuracy-  0.8347067448680352\n",
      "Precision- 0.7120606703090597\n",
      "Recall- 0.31470068836270587\n",
      "F1 Score-  0.43649087728067976\n",
      "\n",
      "\n",
      "Results After Smoothening- \n",
      "Frequency-\n",
      "Accuracy-  0.7567815249266863\n",
      "Precision- 0.6702865803634745\n",
      "Recall- 0.8359055281958798\n",
      "F1 Score-  0.7439904309912412\n",
      "\n",
      "\n",
      "Amplitude-\n",
      "Accuracy-  0.8347507331378299\n",
      "Precision- 0.712721627583953\n",
      "Recall- 0.3143763289725015\n",
      "F1 Score-  0.4363027059470815\n"
     ]
    }
   ],
   "source": [
    "get_metrics_segment_wise(teacher_id,c,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency- \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390d093da6034c23851ede026d6ab537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP- 99, FP- 92, FN- 1, FP- 92\n",
      "The Accuracy is 49.25373134328358%.\n",
      "The Precision is 51.832460732984295%.\n",
      "The Recall is 99.0%.\n",
      "The F1-score is 68.04123711340206%.\n",
      "\n",
      "\n",
      "Amplitude- \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24e142224384620a366692866306808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP- 11, FP- 4, FN- 0, FP- 4\n",
      "The Accuracy is 47.82608695652174%.\n",
      "The Precision is 73.33333333333333%.\n",
      "The Recall is 100.0%.\n",
      "The F1-score is 84.6153846153846%.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4782608695652174, 0.7333333333333333, 1.0, 0.846153846153846)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Frequency- \")\n",
    "get_scores(teacher_id, kappa = 1,metrics_for = 'freq',t_collar = 0, eta = 0.2)\n",
    "print(\"\\n\")\n",
    "print(\"Amplitude- \")\n",
    "get_scores(teacher_id, kappa = 1,metrics_for = 'amp',t_collar = 0, eta = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
