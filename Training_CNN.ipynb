{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.layers import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import sklearn\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(arr1,arr2,arr3,size):\n",
    "    arr_1_n=[]\n",
    "    arr_2_n=[]\n",
    "    arr_3_n=[]\n",
    "    for i in range(len(arr1)):\n",
    "        arr_1_n.append(np.array(divide_and_split_data(arr1[i],size),dtype=object))\n",
    "        arr_2_n.append(np.array(divide_and_split_data(arr2[i],size),dtype=object))\n",
    "        arr_3_n.append(np.array(divide_and_split_data(arr3[i],size),dtype=object))\n",
    "    return np.array(arr_1_n,dtype=object),np.array(arr_2_n,dtype=object),np.array(arr_3_n,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_arr(arr):\n",
    "    arr_n=[]\n",
    "    for i in range(len(arr)):\n",
    "        for j in range(len(arr[i])):\n",
    "            arr_n.append(arr[i][j])\n",
    "    return np.array(arr_n,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_final_data(t_f_1,s_f_1,c_1,t_f_2,s_f_2,c_2):\n",
    "    l1=len(t_f_1)\n",
    "    l2=len(t_f_2)\n",
    "    if l1<l2:\n",
    "        return t_f_1,s_f_1,c_1,t_f_2[:l1],s_f_2[:l1],c_2[:l1]\n",
    "    elif l1>l2:\n",
    "        return t_f_1[:l2],s_f_1[:l2],c_1[:l2],t_f_2,s_f_2,c_2\n",
    "    else:\n",
    "        return t_f_1,s_f_1,c_1,t_f_2,s_f_2,c_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    s=[]\n",
    "    for i in arr:\n",
    "        s.append(np.amax(i))\n",
    "    return arr/max(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_features_and_output(t_f_1,s_f_1,t_f_2,s_f_2,c_1,c_2):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    p=len(t_f_1)\n",
    "    q=len(t_f_2)\n",
    "    if q<p:\n",
    "        for i in range(q):\n",
    "            x.append(np.vstack((t_f_1[i],s_f_1[i],t_f_2[i],s_f_2[i])))\n",
    "            y.append(np.vstack((c_1[i],c_2[i])))\n",
    "    else:\n",
    "        for i in range(p):\n",
    "            x.append(np.vstack((t_f_1[i],s_f_1[i],t_f_2[i],s_f_2[i])))\n",
    "            y.append(np.vstack((c_1[i],c_2[i])))\n",
    "        \n",
    "    return np.array(x,dtype=object),np.array(y,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_and_split_data(arr,size):\n",
    "    if len(arr)==size:\n",
    "        return arr\n",
    "    val=len(arr)%size\n",
    "    pad=size-val\n",
    "    arr_n=np.append(arr,[0]*pad)\n",
    "    return np.split(arr_n, len(arr_n)/size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_and_Y(t_feature_1_padded,s_feature_1_padded,feature_1_class,t_feature_2_padded,s_feature_2_padded,feature_2_class,size):\n",
    "    t_f_1,s_f_1,c_1=prepare_data(t_feature_1_padded,s_feature_1_padded,feature_1_class,size)\n",
    "    t_f_2,s_f_2,c_2=prepare_data(t_feature_2_padded,s_feature_2_padded,feature_2_class,size)\n",
    "\n",
    "    t_f_1,s_f_1,c_1=merge_arr(t_f_1),merge_arr(s_f_1),merge_arr(c_1)\n",
    "    t_f_2,s_f_2,c_2=merge_arr(t_f_2),merge_arr(s_f_2),merge_arr(c_2)\n",
    "    \n",
    "\n",
    "\n",
    "    t_f_1,s_f_1,c_1,t_f_2,s_f_2,c_2=filter_final_data(t_f_1,s_f_1,c_1,t_f_2,s_f_2,c_2)\n",
    "    \n",
    "    \n",
    "    p=c_1\n",
    "    q=c_2\n",
    "    t_f_1,s_f_1,t_f_2,s_f_2=normalize(t_f_1),normalize(s_f_1),normalize(t_f_2),normalize(s_f_2)\n",
    "\n",
    "    X,Y=stack_features_and_output(t_f_1,s_f_1,t_f_2,s_f_2,c_1,c_2)\n",
    "\n",
    "    X = np.asarray(X).astype(np.float32)\n",
    "    Y = np.asarray(Y).astype(np.float32)\n",
    "    X=X.transpose((0,2,1))\n",
    "    Y=Y.transpose((0,2,1))\n",
    "    \n",
    "    return X,Y,p,q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_count(arr):\n",
    "    a1=[]\n",
    "    for i in arr:\n",
    "        \n",
    "        a1.append(sum(i==1))\n",
    "    return sum(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_zero_one_count(arr):\n",
    "    a1=[]\n",
    "    a0=[]\n",
    "    for i in arr:\n",
    "        a1.append(sum(i==1))\n",
    "        a0.append(sum(i==0))\n",
    "    return sum(a1),sum(a0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weights(c1,c2):\n",
    "    return [c1/(c1+c2),c2/(c1+c2)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_avg_weight(x,y,N):\n",
    "    return [x/N,y/N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_idx(arr,size):\n",
    "    samples=math.floor(len(arr)*size)\n",
    "    idx=random.sample(range(0, len(arr)), samples)\n",
    "    return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(arr,idx):\n",
    "    train=[]\n",
    "    test=[]\n",
    "    for i in range(len(arr)):\n",
    "        if i in idx:\n",
    "            test.append(arr[i])\n",
    "        else:\n",
    "            train.append(arr[i])\n",
    "    return np.array(train,dtype=object),np.array(test,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train2(arr,idx):\n",
    "    size=0.10\n",
    "    samples=math.floor(len(arr)*size)\n",
    "    x=len(arr)-samples\n",
    "    return np.array(arr[:x],dtype=object),np.array(arr[x+1:],dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = keras.models.load_model(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1_001=np.load(\"Data/001/features/t_1_001.npy\",allow_pickle=True)\n",
    "s_1_001=np.load(\"Data/001/features/s_1_001.npy\",allow_pickle=True)\n",
    "c_1_001=np.load(\"Data/001/features/c_1_001.npy\",allow_pickle=True)\n",
    "t_2_001=np.load(\"Data/001/features/t_2_001.npy\",allow_pickle=True)\n",
    "s_2_001=np.load(\"Data/001/features/s_2_001.npy\",allow_pickle=True)\n",
    "c_2_001=np.load(\"Data/001/features/c_2_001.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1_002=np.load(\"Data/002/features/t_1_002.npy\",allow_pickle=True)\n",
    "s_1_002=np.load(\"Data/002/features/s_1_002.npy\",allow_pickle=True)\n",
    "c_1_002=np.load(\"Data/002/features/c_1_002.npy\",allow_pickle=True)\n",
    "t_2_002=np.load(\"Data/002/features/t_2_002.npy\",allow_pickle=True)\n",
    "s_2_002=np.load(\"Data/002/features/s_2_002.npy\",allow_pickle=True)\n",
    "c_2_002=np.load(\"Data/002/features/c_2_002.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_001=get_random_idx(t_1_001,0.10)\n",
    "idx_002=get_random_idx(t_1_002,0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1_001_train,t_1_001_test=split_test_train2(t_1_001,idx_001)\n",
    "s_1_001_train,s_1_001_test=split_test_train2(s_1_001,idx_001)\n",
    "c_1_001_train,c_1_001_test=split_test_train2(c_1_001,idx_001)\n",
    "t_2_001_train,t_2_001_test=split_test_train2(t_2_001,idx_001)\n",
    "s_2_001_train,s_2_001_test=split_test_train2(s_2_001,idx_001)\n",
    "c_2_001_train,c_2_001_test=split_test_train2(c_2_001,idx_001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1_002_train,t_1_002_test=split_test_train2(t_1_002,idx_002)\n",
    "s_1_002_train,s_1_002_test=split_test_train2(s_1_002,idx_002)\n",
    "c_1_002_train,c_1_002_test=split_test_train2(c_1_002,idx_002)\n",
    "t_2_002_train,t_2_002_test=split_test_train2(t_2_002,idx_002)\n",
    "s_2_002_train,s_2_002_test=split_test_train2(s_2_002,idx_002)\n",
    "c_2_002_train,c_2_002_test=split_test_train2(c_2_002,idx_002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_001_train,Y_001_train,c1_train,c2_train=generate_X_and_Y(t_1_001_train,s_1_001_train,c_1_001_train,t_2_001_train,s_2_001_train,c_2_001_train,size)\n",
    "print(\"Teacher - 001\")\n",
    "print(\"Train-\")\n",
    "print(X_001_train.shape)\n",
    "print(Y_001_train.shape)\n",
    "X_001_test,Y_001_test,c1_test,c2_test=generate_X_and_Y(t_1_001_test,s_1_001_test,c_1_001_test,t_2_001_test,s_2_001_test,c_2_001_test,size)\n",
    "print(\"Test-\")\n",
    "print(X_001_test.shape)\n",
    "print(Y_001_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_002_train,Y_002_train,p1_train,p2_train=generate_X_and_Y(t_1_002_train,s_1_002_train,c_1_002_train,t_2_002_train,s_2_002_train,c_2_002_train,size)\n",
    "print(\"Teacher - 002\")\n",
    "print(\"Train-\")\n",
    "print(X_002_train.shape)\n",
    "print(Y_002_train.shape)\n",
    "X_002_test,Y_002_test,p1_test,p2_test=generate_X_and_Y(t_1_002_test,s_1_002_test,c_1_002_test,t_2_002_test,s_2_002_test,c_2_002_test,size)\n",
    "print(\"Test-\")\n",
    "print(X_002_test.shape)\n",
    "print(Y_002_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,y_train=X_001_train,Y_001_train\n",
    "# x_test,y_test=X_001_test,Y_001_test\n",
    "# s1=class_count(c1_train)\n",
    "# s2=class_count(c2_train)\n",
    "# t_id=\"001\"\n",
    "\n",
    "x_train,y_train=X_002_train,Y_002_train\n",
    "x_test,y_test=X_002_test,Y_002_test\n",
    "s1=class_count(p1_train)\n",
    "s2=class_count(p2_train)\n",
    "t_id=\"002\"\n",
    "\n",
    "# x_train,y_train=np.concatenate((X_001_train,X_002_train),axis=0),np.concatenate((Y_001_train,Y_002_train))\n",
    "# x_test,y_test=np.concatenate((X_001_test,X_002_test),axis=0),np.concatenate((Y_001_test,Y_002_test))\n",
    "# q1,q2=np.concatenate((c1_train,p1_train)),np.concatenate((c2_train,p2_train))\n",
    "# s1=class_count(q1)\n",
    "# s2=class_count(q2)\n",
    "# np.save(\"x_train_002.npy\",x_train)\n",
    "# np.save(\"y_train_002.npy\",y_train)\n",
    "# np.save(\"x_test_002.npy\",x_test)\n",
    "# np.save(\"y_test_002.npy\",y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw=class_weights(s1,s2)\n",
    "cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape) \n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(size,4),padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu',padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu',padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu',padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu',padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu',padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu',padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu',padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=16, kernel_size=3, activation='relu',padding='same',use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=2, kernel_size=3, activation='sigmoid',padding='same',use_bias=True))\n",
    "#opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train,y_train,epochs=50,verbose=1,class_weight=cw,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=model.evaluate(x_test,y_test,batch_size=64)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=torch.Tensor(model.predict(x_test))\n",
    "y_true=torch.Tensor(y_test)\n",
    "threshold=0.5\n",
    "for i in range(y_pred.shape[0]):\n",
    "    y0 = y_pred[i,:,0]\n",
    "    y1 = y_pred[i,:,1]\n",
    "    y_pred[i,:,0] = (y0>threshold).float()\n",
    "    y_pred[i,:,1] = (y1>threshold).float()\n",
    "y_pred.shape\n",
    "y0 = y_pred[:,:,0]\n",
    "y1 = y_pred[:,:,1]\n",
    "y0 = y0.reshape(-1)\n",
    "y1 = y1.reshape(-1)\n",
    "\n",
    "yA = y_true[:,:,0]\n",
    "yB = y_true[:,:,1]\n",
    "yA = yA.reshape(-1)\n",
    "yB = yB.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy-\",sklearn.metrics.accuracy_score(yA,y0))\n",
    "print(\"F1 Score-\",sklearn.metrics.f1_score(yA,y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy-\",sklearn.metrics.accuracy_score(yB,y1))\n",
    "print(\"F1 Score-\",sklearn.metrics.f1_score(yB,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/model_\"+ t_id +\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"models/model_\"+ t_id +\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "y_true=torch.Tensor(y_test)\n",
    "threshold=0.5\n",
    "for i in range(y_pred.shape[0]):\n",
    "    y0 = y_pred[i,:,0]\n",
    "    y1 = y_pred[i,:,1]\n",
    "    y_pred[i,:,0] = (y0>threshold).float()\n",
    "    y_pred[i,:,1] = (y1>threshold).float()\n",
    "y_pred.shape\n",
    "y0 = y_pred[:,:,0]\n",
    "y1 = y_pred[:,:,1]\n",
    "y0 = y0.reshape(-1)\n",
    "y1 = y1.reshape(-1)\n",
    "\n",
    "yA = y_true[:,:,0]\n",
    "yB = y_true[:,:,1]\n",
    "yA = yA.reshape(-1)\n",
    "yB = yB.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"predictions/y_pred_1_\"+t_id+\".npy\",y0)\n",
    "np.save(\"predictions/y_true_1_\"+t_id+\".npy\",yA)\n",
    "np.save(\"predictions/y_pred_2_\"+t_id+\".npy\",y1)\n",
    "np.save(\"predictions/y_true_2_\"+t_id+\".npy\",yB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
